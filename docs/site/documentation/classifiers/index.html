<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Classifiers - Techtonique/nnetsauce</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/rainbow.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

            ga('create', 'UA-146345113-2', 'https://techtonique.github.io/nnetsauce/');
            ga('send', 'pageview');
        </script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-dark">
            <div class="container">
                <a class="navbar-brand" href="../..">Techtonique/nnetsauce</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Quickstart examples <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../examples/classification/" class="dropdown-item">Classification</a>
</li>
                                    
<li>
    <a href="../../examples/regression/" class="dropdown-item">Regression</a>
</li>
                                    
<li>
    <a href="../../examples/time_series_examples/" class="dropdown-item">Time series</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Documentation <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="./" class="dropdown-item active">Classifiers</a>
</li>
                                    
<li>
    <a href="../regressors/" class="dropdown-item">Regressors</a>
</li>
                                    
<li>
    <a href="../time_series/" class="dropdown-item">Time series</a>
</li>
                                    
<li>
    <a href="../scoring_metrics/" class="dropdown-item">Scoring metrics</a>
</li>
                                </ul>
                            </li>
                            <li class="navitem">
                                <a href="../../CONTRIBUTING/" class="nav-link">Contributing</a>
                            </li>
                            <li class="navitem">
                                <a href="../../LICENSE/" class="nav-link">License</a>
                            </li>
                            <li class="navitem">
                                <a href="../../REFERENCES/" class="nav-link">References</a>
                            </li>
                            <li class="navitem">
                                <a href="../../" class="nav-link">Techtonique</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../examples/time_series_examples/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../regressors/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/Techtonique/nnetsauce" class="nav-link"><i class="fa fa-github"></i> GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#classifiers" class="nav-link">Classifiers</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="classifiers">Classifiers</h1>
<p><em>In alphabetical order</em></p>
<p>All models possess methods <code>fit</code>, <code>predict</code>, <code>predict_proba</code>, and <code>score</code>. For scoring metrics, refer to <a href="../scoring_metrics/">scoring metrics</a>.</p>
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/boosting/adaBoostClassifier.py#L18">[source]</a></span></p>
<h3 id="adaboostclassifier">AdaBoostClassifier</h3>
<pre><code class="language-python">nnetsauce.AdaBoostClassifier(
    obj,
    n_estimators=10,
    learning_rate=0.1,
    n_hidden_features=1,
    reg_lambda=0,
    reg_alpha=0.5,
    activation_name=&quot;relu&quot;,
    a=0.01,
    nodes_sim=&quot;sobol&quot;,
    bias=True,
    dropout=0,
    direct_link=False,
    n_clusters=2,
    cluster_encode=True,
    type_clust=&quot;kmeans&quot;,
    type_scaling=(&quot;std&quot;, &quot;std&quot;, &quot;std&quot;),
    col_sample=1,
    row_sample=1,
    seed=123,
    verbose=1,
    method=&quot;SAMME&quot;,
    backend=&quot;cpu&quot;,
)
</code></pre>
<p>AdaBoost Classification (SAMME) model class derived from class Boosting</p>
<p>Parameters:</p>
<pre><code>obj: object
    any object containing a method fit (obj.fit()) and a method predict
    (obj.predict())

n_estimators: int
    number of boosting iterations

learning_rate: float
    learning rate of the boosting procedure

n_hidden_features: int
    number of nodes in the hidden layer

reg_lambda: float
    regularization parameter for weights

reg_alpha: float
    controls compromize between l1 and l2 norm of weights

activation_name: str
    activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'

a: float
    hyperparameter for 'prelu' or 'elu' activation function

nodes_sim: str
    type of simulation for the nodes: 'sobol', 'hammersley', 'halton',
    'uniform'

bias: boolean
    indicates if the hidden layer contains a bias term (True) or not
    (False)

dropout: float
    regularization parameter; (random) percentage of nodes dropped out
    of the training

direct_link: boolean
    indicates if the original predictors are included (True) in model's
    fitting or not (False)

n_clusters: int
    number of clusters for 'kmeans' or 'gmm' clustering (could be 0:
        no clustering)

cluster_encode: bool
    defines how the variable containing clusters is treated (default is one-hot)
    if `False`, then labels are used, without one-hot encoding

type_clust: str
    type of clustering method: currently k-means ('kmeans') or Gaussian
    Mixture Model ('gmm')

type_scaling: a tuple of 3 strings
    scaling methods for inputs, hidden layer, and clustering respectively
    (and when relevant).
    Currently available: standardization ('std') or MinMax scaling ('minmax')

col_sample: float
    percentage of covariates randomly chosen for training

row_sample: float
    percentage of rows chosen for training, by stratified bootstrapping

seed: int
    reproducibility seed for nodes_sim=='uniform'

method: str
    type of Adaboost method, 'SAMME' (discrete) or 'SAMME.R' (real)

backend: str
    "cpu" or "gpu" or "tpu"
</code></pre>
<p>Attributes:</p>
<pre><code>alpha_: list
    AdaBoost coefficients alpha_m

base_learners_: dict
    a dictionary containing the base learners
</code></pre>
<p>Examples:</p>
<p>See also <a href="https://github.com/Techtonique/nnetsauce/blob/master/examples/adaboost_classification.py">https://github.com/Techtonique/nnetsauce/blob/master/examples/adaboost_classification.py</a></p>
<pre><code class="language-python">import nnetsauce as ns
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from time import time

breast_cancer = load_breast_cancer()
Z = breast_cancer.data
t = breast_cancer.target
np.random.seed(123)
X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2)

# SAMME.R
clf = LogisticRegression(solver='liblinear', multi_class = 'ovr',
                        random_state=123)
fit_obj = ns.AdaBoostClassifier(clf,
                                n_hidden_features=int(11.22338867),
                                direct_link=True,
                                n_estimators=250, learning_rate=0.01126343,
                                col_sample=0.72684326, row_sample=0.86429443,
                                dropout=0.63078613, n_clusters=2,
                                type_clust=&quot;gmm&quot;,
                                verbose=1, seed = 123,
                                method=&quot;SAMME.R&quot;)

start = time()
fit_obj.fit(X_train, y_train)
print(f&quot;Elapsed {time() - start}&quot;)

start = time()
print(fit_obj.score(X_test, y_test))
print(f&quot;Elapsed {time() - start}&quot;)

preds = fit_obj.predict(X_test)

print(fit_obj.score(X_test, y_test, scoring=&quot;roc_auc&quot;))
print(metrics.classification_report(preds, y_test))

</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/boosting/adaBoostClassifier.py#L211">[source]</a></span></p>
<h3 id="fit">fit</h3>
<pre><code class="language-python">AdaBoostClassifier.fit(X, y, sample_weight=None, **kwargs)
</code></pre>
<p>Fit Boosting model to training data (X, y).</p>
<p>Parameters:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

y: array-like, shape = [n_samples]
    Target values.

**kwargs: additional parameters to be passed to
        self.cook_training_set or self.obj.fit
</code></pre>
<p>Returns:</p>
<pre><code> self: object
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/boosting/adaBoostClassifier.py#L382">[source]</a></span></p>
<h3 id="predict">predict</h3>
<pre><code class="language-python">AdaBoostClassifier.predict(X, **kwargs)
</code></pre>
<p>Predict test data X.</p>
<p>Parameters:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

**kwargs: additional parameters to be passed to
      self.cook_test_set
</code></pre>
<p>Returns:</p>
<pre><code>model predictions: {array-like}
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/boosting/adaBoostClassifier.py#L401">[source]</a></span></p>
<h3 id="predict_proba">predict_proba</h3>
<pre><code class="language-python">AdaBoostClassifier.predict_proba(X, **kwargs)
</code></pre>
<p>Predict probabilities for test data X.</p>
<p>Parameters:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

**kwargs: additional parameters to be passed to
      self.cook_test_set
</code></pre>
<p>Returns:</p>
<pre><code>probability estimates for test data: {array-like}
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/boosting/adaBoostClassifier.py#L480">[source]</a></span></p>
<h3 id="score">score</h3>
<pre><code class="language-python">AdaBoostClassifier.score(X, y, scoring=None, **kwargs)
</code></pre>
<p>Score the model on test set features X and response y.</p>
<p>Parameters:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features

y: array-like, shape = [n_samples]
    Target values

scoring: str
    must be in ('accuracy', 'average_precision',
               'brier_score_loss', 'f1', 'f1_micro',
               'f1_macro', 'f1_weighted',  'f1_samples',
               'neg_log_loss', 'precision', 'recall',
               'roc_auc')

**kwargs: additional parameters to be passed to scoring functions
</code></pre>
<p>Returns:</p>
<pre><code>model scores: {array-like}
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/custom/customClassifier.py#L12">[source]</a></span></p>
<h3 id="customclassifier">CustomClassifier</h3>
<pre><code class="language-python">nnetsauce.CustomClassifier(
    obj,
    n_hidden_features=5,
    activation_name=&quot;relu&quot;,
    a=0.01,
    nodes_sim=&quot;sobol&quot;,
    bias=True,
    dropout=0,
    direct_link=True,
    n_clusters=2,
    cluster_encode=True,
    type_clust=&quot;kmeans&quot;,
    type_scaling=(&quot;std&quot;, &quot;std&quot;, &quot;std&quot;),
    col_sample=1,
    row_sample=1,
    seed=123,
    backend=&quot;cpu&quot;,
)
</code></pre>
<p>Custom Classification model</p>
<p>Attributes:</p>
<pre><code>obj: object
    any object containing a method fit (obj.fit()) and a method predict
    (obj.predict())

n_hidden_features: int
    number of nodes in the hidden layer

activation_name: str
    activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'

a: float
    hyperparameter for 'prelu' or 'elu' activation function

nodes_sim: str
    type of simulation for the nodes: 'sobol', 'hammersley', 'halton',
    'uniform'

bias: boolean
    indicates if the hidden layer contains a bias term (True) or not
    (False)

dropout: float
    regularization parameter; (random) percentage of nodes dropped out
    of the training

direct_link: boolean
    indicates if the original predictors are included (True) in model''s
    fitting or not (False)

n_clusters: int
    number of clusters for 'kmeans' or 'gmm' clustering (could be 0:
        no clustering)

cluster_encode: bool
    defines how the variable containing clusters is treated (default is one-hot)
    if `False`, then labels are used, without one-hot encoding

type_clust: str
    type of clustering method: currently k-means ('kmeans') or Gaussian
    Mixture Model ('gmm')

type_scaling: a tuple of 3 strings
    scaling methods for inputs, hidden layer, and clustering respectively
    (and when relevant).
    Currently available: standardization ('std') or MinMax scaling ('minmax')

col_sample: float
    percentage of covariates randomly chosen for training

row_sample: float
    percentage of rows chosen for training, by stratified bootstrapping

seed: int
    reproducibility seed for nodes_sim=='uniform'

backend: str
    "cpu" or "gpu" or "tpu"
</code></pre>
<p>Examples:</p>
<pre><code class="language-python">import nnetsauce as ns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
from time import time

digits = load_digits()
X = digits.data
y = digits.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=123)

# layer 1 (base layer) ----
layer1_regr = RandomForestClassifier(n_estimators=10, random_state=123)

start = time()

layer1_regr.fit(X_train, y_train)

# Accuracy in layer 1
print(layer1_regr.score(X_test, y_test))

# layer 2 using layer 1 ----
layer2_regr = ns.CustomClassifier(obj = layer1_regr, n_hidden_features=5,
                        direct_link=True, bias=True,
                        nodes_sim='uniform', activation_name='relu',
                        n_clusters=2, seed=123)
layer2_regr.fit(X_train, y_train)

# Accuracy in layer 2
print(layer2_regr.score(X_test, y_test))

# layer 3 using layer 2 ----
layer3_regr = ns.CustomClassifier(obj = layer2_regr, n_hidden_features=10,
                        direct_link=True, bias=True, dropout=0.7,
                        nodes_sim='uniform', activation_name='relu',
                        n_clusters=2, seed=123)
layer3_regr.fit(X_train, y_train)

# Accuracy in layer 3
print(layer3_regr.score(X_test, y_test))

print(f&quot;Elapsed {time() - start}&quot;)
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/custom/customClassifier.py#L168">[source]</a></span></p>
<h3 id="fit_1">fit</h3>
<pre><code class="language-python">CustomClassifier.fit(X, y, sample_weight=None, **kwargs)
</code></pre>
<p>Fit custom model to training data (X, y).</p>
<p>Parameters:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

y: array-like, shape = [n_samples]
    Target values.

**kwargs: additional parameters to be passed to
            self.cook_training_set or self.obj.fit
</code></pre>
<p>Returns:</p>
<pre><code>self: object
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/glm/glmClassifier.py#L18">[source]</a></span></p>
<h3 id="glmclassifier">GLMClassifier</h3>
<pre><code class="language-python">nnetsauce.GLMClassifier(
    n_hidden_features=5,
    lambda1=0.01,
    alpha1=0.5,
    lambda2=0.01,
    alpha2=0.5,
    family=&quot;expit&quot;,
    activation_name=&quot;relu&quot;,
    a=0.01,
    nodes_sim=&quot;sobol&quot;,
    bias=True,
    dropout=0,
    direct_link=True,
    n_clusters=2,
    cluster_encode=True,
    type_clust=&quot;kmeans&quot;,
    type_scaling=(&quot;std&quot;, &quot;std&quot;, &quot;std&quot;),
    optimizer=Optimizer(),
    seed=123,
)
</code></pre>
<p>Generalized 'linear' models using quasi-randomized networks (classification)</p>
<p>Parameters:</p>
<pre><code>n_hidden_features: int
    number of nodes in the hidden layer

lambda1: float
    regularization parameter for GLM coefficients on original features

alpha1: float
    controls compromize between l1 and l2 norm of GLM coefficients on original features

lambda2: float
    regularization parameter for GLM coefficients on nonlinear features

alpha2: float
    controls compromize between l1 and l2 norm of GLM coefficients on nonlinear features

activation_name: str
    activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'

a: float
    hyperparameter for 'prelu' or 'elu' activation function

nodes_sim: str
    type of simulation for the nodes: 'sobol', 'hammersley', 'halton',
    'uniform'

bias: boolean
    indicates if the hidden layer contains a bias term (True) or not
    (False)

dropout: float
    regularization parameter; (random) percentage of nodes dropped out
    of the training

direct_link: boolean
    indicates if the original predictors are included (True) in model's
    fitting or not (False)

n_clusters: int
    number of clusters for 'kmeans' or 'gmm' clustering (could be 0:
        no clustering)

cluster_encode: bool
    defines how the variable containing clusters is treated (default is one-hot)
    if `False`, then labels are used, without one-hot encoding

type_clust: str
    type of clustering method: currently k-means ('kmeans') or Gaussian
    Mixture Model ('gmm')

type_scaling: a tuple of 3 strings
    scaling methods for inputs, hidden layer, and clustering respectively
    (and when relevant).
    Currently available: standardization ('std') or MinMax scaling ('minmax')

optimizer: object
    optimizer, from class nnetsauce.utils.Optimizer

seed: int
    reproducibility seed for nodes_sim=='uniform'
</code></pre>
<p>Attributes:</p>
<pre><code>beta_: vector
    regression coefficients
</code></pre>
<p>Examples:</p>
<p>See <a href="https://github.com/Techtonique/nnetsauce/blob/master/examples/glm_classification.py">https://github.com/Techtonique/nnetsauce/blob/master/examples/glm_classification.py</a></p>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/glm/glmClassifier.py#L207">[source]</a></span></p>
<h3 id="fit_2">fit</h3>
<pre><code class="language-python">GLMClassifier.fit(
    X, y, learning_rate=0.01, decay=0.1, batch_prop=1, tolerance=1e-05, optimizer=None, verbose=1, **kwargs
)
</code></pre>
<p>Fit GLM model to training data (X, y).</p>
<p>Args:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

y: array-like, shape = [n_samples]
    Target values.

**kwargs: additional parameters to be passed to
        self.cook_training_set or self.obj.fit
</code></pre>
<p>Returns:</p>
<pre><code>self: object
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/multitask/multitaskClassifier.py#L15">[source]</a></span></p>
<h3 id="multitaskclassifier">MultitaskClassifier</h3>
<pre><code class="language-python">nnetsauce.MultitaskClassifier(
    obj,
    n_hidden_features=5,
    activation_name=&quot;relu&quot;,
    a=0.01,
    nodes_sim=&quot;sobol&quot;,
    bias=True,
    dropout=0,
    direct_link=True,
    n_clusters=2,
    cluster_encode=True,
    type_clust=&quot;kmeans&quot;,
    type_scaling=(&quot;std&quot;, &quot;std&quot;, &quot;std&quot;),
    col_sample=1,
    row_sample=1,
    seed=123,
    backend=&quot;cpu&quot;,
)
</code></pre>
<p>Multitask Classification model based on regression models, with shared covariates</p>
<p>Parameters:</p>
<pre><code>obj: object
    any object (must be a regression model) containing a method fit (obj.fit())
    and a method predict (obj.predict())

n_hidden_features: int
    number of nodes in the hidden layer

activation_name: str
    activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'

a: float
    hyperparameter for 'prelu' or 'elu' activation function

nodes_sim: str
    type of simulation for the nodes: 'sobol', 'hammersley', 'halton',
    'uniform'

bias: boolean
    indicates if the hidden layer contains a bias term (True) or not
    (False)

dropout: float
    regularization parameter; (random) percentage of nodes dropped out
    of the training

direct_link: boolean
    indicates if the original predictors are included (True) in model's
    fitting or not (False)

n_clusters: int
    number of clusters for 'kmeans' or 'gmm' clustering (could be 0:
        no clustering)

cluster_encode: bool
    defines how the variable containing clusters is treated (default is one-hot)
    if `False`, then labels are used, without one-hot encoding

type_clust: str
    type of clustering method: currently k-means ('kmeans') or Gaussian
    Mixture Model ('gmm')

type_scaling: a tuple of 3 strings
    scaling methods for inputs, hidden layer, and clustering respectively
    (and when relevant).
    Currently available: standardization ('std') or MinMax scaling ('minmax')

col_sample: float
    percentage of covariates randomly chosen for training

row_sample: float
    percentage of rows chosen for training, by stratified bootstrapping

seed: int
    reproducibility seed for nodes_sim=='uniform'

backend: str
    "cpu" or "gpu" or "tpu"
</code></pre>
<p>Attributes:</p>
<pre><code>fit_objs_: dict
    objects adjusted to each individual time series

n_classes_: int
    number of classes for the classifier
</code></pre>
<p>Examples:</p>
<p>See also <a href="https://github.com/Techtonique/nnetsauce/blob/master/examples/mtask_classification.py">https://github.com/Techtonique/nnetsauce/blob/master/examples/mtask_classification.py</a></p>
<pre><code class="language-python">import nnetsauce as ns
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from time import time

breast_cancer = load_breast_cancer()
Z = breast_cancer.data
t = breast_cancer.target

X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2,
                                                    random_state=123+2*10)

# Linear Regression is used
regr = LinearRegression()
fit_obj = ns.MultitaskClassifier(regr, n_hidden_features=5,
                            n_clusters=2, type_clust=&quot;gmm&quot;)

start = time()
fit_obj.fit(X_train, y_train)
print(f&quot;Elapsed {time() - start}&quot;)

print(fit_obj.score(X_test, y_test))
print(fit_obj.score(X_test, y_test, scoring=&quot;roc_auc&quot;))

start = time()
preds = fit_obj.predict(X_test)
print(f&quot;Elapsed {time() - start}&quot;)
print(metrics.classification_report(preds, y_test))
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/multitask/multitaskClassifier.py#L170">[source]</a></span></p>
<h3 id="fit_3">fit</h3>
<pre><code class="language-python">MultitaskClassifier.fit(X, y, sample_weight=None, **kwargs)
</code></pre>
<p>Fit MultitaskClassifier to training data (X, y).</p>
<p>Args:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

y: array-like, shape = [n_samples]
    Target values.

**kwargs: additional parameters to be passed to
        self.cook_training_set or self.obj.fit
</code></pre>
<p>Returns:</p>
<pre><code>self: object
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/randombag/_randomBagClassifier.py#L18">[source]</a></span></p>
<h3 id="randombagclassifier">RandomBagClassifier</h3>
<pre><code class="language-python">nnetsauce.RandomBagClassifier(
    obj,
    n_estimators=10,
    n_hidden_features=1,
    activation_name=&quot;relu&quot;,
    a=0.01,
    nodes_sim=&quot;sobol&quot;,
    bias=True,
    dropout=0,
    direct_link=False,
    n_clusters=2,
    cluster_encode=True,
    type_clust=&quot;kmeans&quot;,
    type_scaling=(&quot;std&quot;, &quot;std&quot;, &quot;std&quot;),
    col_sample=1,
    row_sample=1,
    n_jobs=None,
    seed=123,
    verbose=1,
    backend=&quot;cpu&quot;,
)
</code></pre>
<p>Randomized 'Bagging' Classification model</p>
<p>Parameters:</p>
<pre><code>obj: object
    any object containing a method fit (obj.fit()) and a method predict
    (obj.predict())

n_estimators: int
    number of boosting iterations

n_hidden_features: int
    number of nodes in the hidden layer

activation_name: str
    activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'

a: float
    hyperparameter for 'prelu' or 'elu' activation function

nodes_sim: str
    type of simulation for the nodes: 'sobol', 'hammersley', 'halton',
    'uniform'

bias: boolean
    indicates if the hidden layer contains a bias term (True) or not
    (False)

dropout: float
    regularization parameter; (random) percentage of nodes dropped out
    of the training

direct_link: boolean
    indicates if the original predictors are included (True) in model's
    fitting or not (False)

n_clusters: int
    number of clusters for 'kmeans' or 'gmm' clustering (could be 0:
        no clustering)

cluster_encode: bool
    defines how the variable containing clusters is treated (default is one-hot)
    if `False`, then labels are used, without one-hot encoding

type_clust: str
    type of clustering method: currently k-means ('kmeans') or Gaussian
    Mixture Model ('gmm')

type_scaling: a tuple of 3 strings
    scaling methods for inputs, hidden layer, and clustering respectively
    (and when relevant).
    Currently available: standardization ('std') or MinMax scaling ('minmax')

col_sample: float
    percentage of covariates randomly chosen for training

row_sample: float
    percentage of rows chosen for training, by stratified bootstrapping

seed: int
    reproducibility seed for nodes_sim=='uniform'

backend: str
    "cpu" or "gpu" or "tpu"
</code></pre>
<p>Attributes:</p>
<pre><code>voter_: dict
    dictionary containing all the fitted base-learners
</code></pre>
<p>Examples:</p>
<p>See also <a href="https://github.com/Techtonique/nnetsauce/blob/master/examples/randombag_classification.py">https://github.com/Techtonique/nnetsauce/blob/master/examples/randombag_classification.py</a></p>
<pre><code class="language-python">import nnetsauce as ns
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from time import time


breast_cancer = load_breast_cancer()
Z = breast_cancer.data
t = breast_cancer.target
np.random.seed(123)
X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2)

# decision tree
clf = DecisionTreeClassifier(max_depth=2, random_state=123)
fit_obj = ns.RandomBagClassifier(clf, n_hidden_features=2,
                                direct_link=True,
                                n_estimators=100,
                                col_sample=0.9, row_sample=0.9,
                                dropout=0.3, n_clusters=0, verbose=1)

start = time()
fit_obj.fit(X_train, y_train)
print(f&quot;Elapsed {time() - start}&quot;)

print(fit_obj.score(X_test, y_test))
print(fit_obj.score(X_test, y_test, scoring=&quot;roc_auc&quot;))

start = time()
preds = fit_obj.predict(X_test)
print(f&quot;Elapsed {time() - start}&quot;)
print(metrics.classification_report(preds, y_test))
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/randombag/_randomBagClassifier.py#L182">[source]</a></span></p>
<h3 id="fit_4">fit</h3>
<pre><code class="language-python">RandomBagClassifier.fit(X, y, **kwargs)
</code></pre>
<p>Fit Random 'Bagging' model to training data (X, y).</p>
<p>Args:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

y: array-like, shape = [n_samples]
    Target values.

**kwargs: additional parameters to be passed to
        self.cook_training_set or self.obj.fit
</code></pre>
<p>Returns:</p>
<pre><code>self: object
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/ridge2/ridge2Classifier.py#L17">[source]</a></span></p>
<h3 id="ridge2classifier">Ridge2Classifier</h3>
<pre><code class="language-python">nnetsauce.Ridge2Classifier(
    n_hidden_features=5,
    activation_name=&quot;relu&quot;,
    a=0.01,
    nodes_sim=&quot;sobol&quot;,
    bias=True,
    dropout=0,
    direct_link=True,
    n_clusters=2,
    cluster_encode=True,
    type_clust=&quot;kmeans&quot;,
    type_scaling=(&quot;std&quot;, &quot;std&quot;, &quot;std&quot;),
    lambda1=0.1,
    lambda2=0.1,
    seed=123,
    backend=&quot;cpu&quot;,
)
</code></pre>
<p>Multinomial logit classification with 2 regularization parameters</p>
<p>Parameters:</p>
<pre><code>n_hidden_features: int
    number of nodes in the hidden layer

activation_name: str
    activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'

a: float
    hyperparameter for 'prelu' or 'elu' activation function

nodes_sim: str
    type of simulation for the nodes: 'sobol', 'hammersley', 'halton',
    'uniform'

bias: boolean
    indicates if the hidden layer contains a bias term (True) or not
    (False)

dropout: float
    regularization parameter; (random) percentage of nodes dropped out
    of the training

direct_link: boolean
    indicates if the original predictors are included (True) in model's
    fitting or not (False)

n_clusters: int
    number of clusters for 'kmeans' or 'gmm' clustering (could be 0:
        no clustering)

cluster_encode: bool
    defines how the variable containing clusters is treated (default is one-hot)
    if `False`, then labels are used, without one-hot encoding

type_clust: str
    type of clustering method: currently k-means ('kmeans') or Gaussian
    Mixture Model ('gmm')

type_scaling: a tuple of 3 strings
    scaling methods for inputs, hidden layer, and clustering respectively
    (and when relevant).
    Currently available: standardization ('std') or MinMax scaling ('minmax')

lambda1: float
    regularization parameter on direct link

lambda2: float
    regularization parameter on hidden layer

seed: int
    reproducibility seed for nodes_sim=='uniform'

backend: str
    "cpu" or "gpu" or "tpu"
</code></pre>
<p>Attributes:</p>
<pre><code>beta_: {array-like}
    regression coefficients
</code></pre>
<p>Examples:</p>
<p>See also <a href="https://github.com/Techtonique/nnetsauce/blob/master/examples/ridge_classification.py">https://github.com/Techtonique/nnetsauce/blob/master/examples/ridge_classification.py</a></p>
<pre><code class="language-python">import nnetsauce as ns
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from time import time


breast_cancer = load_breast_cancer()
X = breast_cancer.data
y = breast_cancer.target

# split data into training test and test set
np.random.seed(123)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# create the model with nnetsauce
fit_obj = ns.Ridge2Classifier(lambda1 = 6.90185578e+04,
                            lambda2 = 3.17392781e+02,
                            n_hidden_features=95,
                            n_clusters=2,
                            dropout = 3.62817383e-01,
                            type_clust = &quot;gmm&quot;)

# fit the model on training set
start = time()
fit_obj.fit(X_train, y_train)
print(f&quot;Elapsed {time() - start}&quot;)

# get the accuracy on test set
start = time()
print(fit_obj.score(X_test, y_test))
print(f&quot;Elapsed {time() - start}&quot;)

# get area under the curve on test set (auc)
print(fit_obj.score(X_test, y_test, scoring=&quot;roc_auc&quot;))
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/ridge2/ridge2Classifier.py#L301">[source]</a></span></p>
<h3 id="fit_5">fit</h3>
<pre><code class="language-python">Ridge2Classifier.fit(X, y, solver=&quot;L-BFGS-B&quot;, **kwargs)
</code></pre>
<p>Fit Ridge model to training data (X, y).</p>
<p>for beta: regression coeffs (beta11, ..., beta1p, ..., betaK1, ..., betaKp)
for K classes and p covariates.</p>
<p>Args:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

y: array-like, shape = [n_samples]
    Target values.

**kwargs: additional parameters to be passed to
        self.cook_training_set or self.obj.fit
</code></pre>
<p>Returns:</p>
<pre><code>self: object
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/ridge2/ridge2MultitaskClassifier.py#L20">[source]</a></span></p>
<h3 id="ridge2multitaskclassifier">Ridge2MultitaskClassifier</h3>
<pre><code class="language-python">nnetsauce.Ridge2MultitaskClassifier(
    n_hidden_features=5,
    activation_name=&quot;relu&quot;,
    a=0.01,
    nodes_sim=&quot;sobol&quot;,
    bias=True,
    dropout=0,
    n_clusters=2,
    cluster_encode=True,
    type_clust=&quot;kmeans&quot;,
    type_scaling=(&quot;std&quot;, &quot;std&quot;, &quot;std&quot;),
    lambda1=0.1,
    lambda2=0.1,
    seed=123,
    backend=&quot;cpu&quot;,
)
</code></pre>
<p>Multitask Ridge classification with 2 regularization parameters</p>
<p>Parameters:</p>
<pre><code>n_hidden_features: int
    number of nodes in the hidden layer

activation_name: str
    activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'

a: float
    hyperparameter for 'prelu' or 'elu' activation function

nodes_sim: str
    type of simulation for the nodes: 'sobol', 'hammersley', 'halton',
    'uniform'

bias: boolean
    indicates if the hidden layer contains a bias term (True) or not
    (False)

dropout: float
    regularization parameter; (random) percentage of nodes dropped out
    of the training

n_clusters: int
    number of clusters for 'kmeans' or 'gmm' clustering (could be 0:
        no clustering)

cluster_encode: bool
    defines how the variable containing clusters is treated (default is one-hot)
    if `False`, then labels are used, without one-hot encoding

type_clust: str
    type of clustering method: currently k-means ('kmeans') or Gaussian
    Mixture Model ('gmm')

type_scaling: a tuple of 3 strings
    scaling methods for inputs, hidden layer, and clustering respectively
    (and when relevant).
    Currently available: standardization ('std') or MinMax scaling ('minmax')

lambda1: float
    regularization parameter on direct link

lambda2: float
    regularization parameter on hidden layer

seed: int
    reproducibility seed for nodes_sim=='uniform'

backend: str
    "cpu" or "gpu" or "tpu"
</code></pre>
<p>Attributes:</p>
<pre><code>beta_: {array-like}
    regression coefficients
</code></pre>
<p>Examples:</p>
<p>See also <a href="https://github.com/Techtonique/nnetsauce/blob/master/examples/ridgemtask_classification.py">https://github.com/Techtonique/nnetsauce/blob/master/examples/ridgemtask_classification.py</a></p>
<pre><code class="language-python">import nnetsauce as ns
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn import metrics
from time import time

breast_cancer = load_breast_cancer()
Z = breast_cancer.data
t = breast_cancer.target
np.random.seed(123)
X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2)

fit_obj = ns.Ridge2MultitaskClassifier(n_hidden_features=int(9.83730469e+01),
                                dropout=4.31054687e-01,
                                n_clusters=int(1.71484375e+00),
                                lambda1=1.24023438e+01, lambda2=7.30263672e+03)

start = time()
fit_obj.fit(X_train, y_train)
print(f&quot;Elapsed {time() - start}&quot;)

print(fit_obj.score(X_test, y_test))
print(fit_obj.score(X_test, y_test, scoring=&quot;roc_auc&quot;))

start = time()
preds = fit_obj.predict(X_test)
print(f&quot;Elapsed {time() - start}&quot;)
print(metrics.classification_report(preds, y_test))
</code></pre>
<hr />
<p><span style="float:right;"><a href="https://github.com/Techtonique/nnetsauce/nnetsauce/ridge2/ridge2MultitaskClassifier.py#L157">[source]</a></span></p>
<h3 id="fit_6">fit</h3>
<pre><code class="language-python">Ridge2MultitaskClassifier.fit(X, y, **kwargs)
</code></pre>
<p>Fit Ridge model to training data (X, y).</p>
<p>Args:</p>
<pre><code>X: {array-like}, shape = [n_samples, n_features]
    Training vectors, where n_samples is the number
    of samples and n_features is the number of features.

y: array-like, shape = [n_samples]
    Target values.

**kwargs: additional parameters to be passed to
        self.cook_training_set or self.obj.fit
</code></pre>
<p>Returns:</p>
<pre><code>self: object
</code></pre>
<hr /></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
