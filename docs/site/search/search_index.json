{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"nnetsauce | Star Welcome to nnetsauce 's website. nnetsauce does Statistical/Machine Learning (ML) using advanced combinations of randomized and quasi-randomized neural networks layers. It contains models for regression, classification, and time series forecasting. Every ML model in nnetsauce is based on components g(XW + b), where: X is a matrix containing explanatory variables and optional clustering information. Clustering the inputs helps in taking into account data\u2019s heterogeneity before model fitting. W creates new, additional explanatory variables from X. W can be drawn from various random and quasirandom sequences. b is an optional bias parameter. g is an activation function such as the hyperbolic tangent or the sigmoid function, that renders the combination of explanatory variables \u2013 through W \u2013 nonlinear. nnetsauce \u2019s source code is available on GitHub . You can read posts about nnetsauce in this blog , and for current references, feel free consult the section: References . Looking for a specific function? You can also use the search function available in the navigation bar. Installing (for Python and R) Python 1st method : by using pip at the command line for the stable version pip install nnetsauce 2nd method : from Github, for the development version pip install git+https://github.com/Techtonique/nnetsauce.git or git clone https://github.com/Techtonique/nnetsauce.git cd nnetsauce make install R 1st method : From Github, in R console: library(devtools) devtools::install_github(\"Techtonique/nnetsauce/R-package\") library(nnetsauce) General rule for using the package in R : object accesses with . 's are replaced by $ 's. See also Quick start . Quickstart Examples of use: For classification For regression For time series R examples can be found in these notebooks: thierrymoudiki_060320_RandomBagClassifier.Rmd thierrymoudiki_060320_Ridge2Classifier.Rmd Documentation The documentation for each model can be found (work in progress) here: For classifiers For regressors For time series models Contributing Want to contribute to nnetsauce 's development on Github, read this !","title":"nnetsauce | <a class=\"github-button\" href=\"https://github.com/Techtonique/nnetsauce/stargazers\" data-color-scheme=\"no-preference: light; light: light; dark: dark;\" data-size=\"large\" aria-label=\"Star nnetsauce/nnetsauce on GitHub\">Star</a>"},{"location":"#nnetsauce-star","text":"Welcome to nnetsauce 's website. nnetsauce does Statistical/Machine Learning (ML) using advanced combinations of randomized and quasi-randomized neural networks layers. It contains models for regression, classification, and time series forecasting. Every ML model in nnetsauce is based on components g(XW + b), where: X is a matrix containing explanatory variables and optional clustering information. Clustering the inputs helps in taking into account data\u2019s heterogeneity before model fitting. W creates new, additional explanatory variables from X. W can be drawn from various random and quasirandom sequences. b is an optional bias parameter. g is an activation function such as the hyperbolic tangent or the sigmoid function, that renders the combination of explanatory variables \u2013 through W \u2013 nonlinear. nnetsauce \u2019s source code is available on GitHub . You can read posts about nnetsauce in this blog , and for current references, feel free consult the section: References . Looking for a specific function? You can also use the search function available in the navigation bar.","title":"nnetsauce | Star"},{"location":"#installing-for-python-and-r","text":"","title":"Installing (for Python and R)"},{"location":"#python","text":"1st method : by using pip at the command line for the stable version pip install nnetsauce 2nd method : from Github, for the development version pip install git+https://github.com/Techtonique/nnetsauce.git or git clone https://github.com/Techtonique/nnetsauce.git cd nnetsauce make install","title":"Python"},{"location":"#r","text":"1st method : From Github, in R console: library(devtools) devtools::install_github(\"Techtonique/nnetsauce/R-package\") library(nnetsauce) General rule for using the package in R : object accesses with . 's are replaced by $ 's. See also Quick start .","title":"R"},{"location":"#quickstart","text":"Examples of use: For classification For regression For time series R examples can be found in these notebooks: thierrymoudiki_060320_RandomBagClassifier.Rmd thierrymoudiki_060320_Ridge2Classifier.Rmd","title":"Quickstart"},{"location":"#documentation","text":"The documentation for each model can be found (work in progress) here: For classifiers For regressors For time series models","title":"Documentation"},{"location":"#contributing","text":"Want to contribute to nnetsauce 's development on Github, read this !","title":"Contributing"},{"location":"CONTRIBUTING/","text":"nnetsauce Code of Conduct 1. Purpose A primary goal of this project is to be inclusive to the largest number of contributors, and most importantly with the most varied and diverse backgrounds possible . As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion, or lack of religion thereof. This code of conduct outlines our expectations for all those who participate to the project, as well as the consequences for unacceptable behavior. We invite all those who participate in, to help us create safe and positive experiences for everyone. 2. Open [Source/Culture/Tech] Citizenship A supplemental goal of this Code of Conduct is to encourage participants to recognize and strengthen the relationships between our actions and their effects on other participants. Communities mirror the societies in which they exist, and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society. 3. Expected Behavior The following behaviors are expected and requested of all contributors: Attempt collaboration before conflict . Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this project. Exercise consideration and respect in your speech and actions. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants. 4. Unacceptable Behavior The following behaviors are considered harassment and are unacceptable: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people's personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Unwelcome sexual attention. This includes, sexualized comments or jokes. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior. 5. Consequences of Unacceptable Behavior Unacceptable behavior from any contributor will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a contributor engages in unacceptable behavior, appropriate action will be taken, up to and including a temporary ban or permanent expulsion without warning. 6. Scope We expect all contributors to abide by this Code of Conduct in all venues, online and in-person. 7. Contact info thierry.moudiki AT gmail.com 8. License and attribution Portions of text derived from the Citizen Code of Conduct .","title":"Contributing"},{"location":"CONTRIBUTING/#nnetsauce-code-of-conduct","text":"","title":"nnetsauce Code of Conduct"},{"location":"CONTRIBUTING/#1-purpose","text":"A primary goal of this project is to be inclusive to the largest number of contributors, and most importantly with the most varied and diverse backgrounds possible . As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion, or lack of religion thereof. This code of conduct outlines our expectations for all those who participate to the project, as well as the consequences for unacceptable behavior. We invite all those who participate in, to help us create safe and positive experiences for everyone.","title":"1. Purpose"},{"location":"CONTRIBUTING/#2-open-sourceculturetech-citizenship","text":"A supplemental goal of this Code of Conduct is to encourage participants to recognize and strengthen the relationships between our actions and their effects on other participants. Communities mirror the societies in which they exist, and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society.","title":"2. Open [Source/Culture/Tech] Citizenship"},{"location":"CONTRIBUTING/#3-expected-behavior","text":"The following behaviors are expected and requested of all contributors: Attempt collaboration before conflict . Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this project. Exercise consideration and respect in your speech and actions. Refrain from demeaning, discriminatory, or harassing behavior and speech. Be mindful of your surroundings and of your fellow participants.","title":"3. Expected Behavior"},{"location":"CONTRIBUTING/#4-unacceptable-behavior","text":"The following behaviors are considered harassment and are unacceptable: Violence, threats of violence or violent language directed against another person. Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language. Posting or displaying sexually explicit or violent material. Posting or threatening to post other people's personally identifying information (\"doxing\"). Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability. Inappropriate photography or recording. Unwelcome sexual attention. This includes, sexualized comments or jokes. Deliberate intimidation, stalking or following (online or in person). Advocating for, or encouraging, any of the above behavior.","title":"4. Unacceptable Behavior"},{"location":"CONTRIBUTING/#5-consequences-of-unacceptable-behavior","text":"Unacceptable behavior from any contributor will not be tolerated. Anyone asked to stop unacceptable behavior is expected to comply immediately. If a contributor engages in unacceptable behavior, appropriate action will be taken, up to and including a temporary ban or permanent expulsion without warning.","title":"5. Consequences of Unacceptable Behavior"},{"location":"CONTRIBUTING/#6-scope","text":"We expect all contributors to abide by this Code of Conduct in all venues, online and in-person.","title":"6. Scope"},{"location":"CONTRIBUTING/#7-contact-info","text":"thierry.moudiki AT gmail.com","title":"7. Contact info"},{"location":"CONTRIBUTING/#8-license-and-attribution","text":"Portions of text derived from the Citizen Code of Conduct .","title":"8. License and attribution"},{"location":"LICENSE/","text":"The Clear BSD License Copyright (c) [2019] [Thierry Moudiki] All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted (subject to the limitations in the disclaimer below) provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. NO EXPRESS OR IMPLIED LICENSES TO ANY PARTY'S PATENT RIGHTS ARE GRANTED BY THIS LICENSE. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"REFERENCES/","text":"Current references - Moudiki, T. (2020). Quasi-randomized networks for regression and classification, with two shrinkage parameters. Available at: https://www.researchgate.net/publication/339512391_Quasi-randomized_networks_for_regression_and_classification_with_two_shrinkage_parameters - Moudiki, T. (2019). Multinomial logistic regression using quasi-randomized networks. Available at: https://www.researchgate.net/publication/334706878_Multinomial_logistic_regression_using_quasi-randomized_networks - Moudiki T, Planchet F, Cousin A (2018). \u201cMultiple Time Series Forecasting Using Quasi-Randomized Functional Link Neural Networks. \u201dRisks, 6(1), 22. Available at: https://www.researchgate.net/publication/323712082_Multiple_Time_Series_Forecasting_Using_Quasi-Randomized_Functional_Link_Neural_Networks","title":"References"},{"location":"REFERENCES/#current-references","text":"- Moudiki, T. (2020). Quasi-randomized networks for regression and classification, with two shrinkage parameters. Available at: https://www.researchgate.net/publication/339512391_Quasi-randomized_networks_for_regression_and_classification_with_two_shrinkage_parameters - Moudiki, T. (2019). Multinomial logistic regression using quasi-randomized networks. Available at: https://www.researchgate.net/publication/334706878_Multinomial_logistic_regression_using_quasi-randomized_networks - Moudiki T, Planchet F, Cousin A (2018). \u201cMultiple Time Series Forecasting Using Quasi-Randomized Functional Link Neural Networks. \u201dRisks, 6(1), 22. Available at: https://www.researchgate.net/publication/323712082_Multiple_Time_Series_Forecasting_Using_Quasi-Randomized_Functional_Link_Neural_Networks","title":"Current references"},{"location":"documentation/classifiers/","text":"Classifiers In alphabetical order All models possess methods fit , predict , predict_proba , and score . For scoring metrics, refer to scoring metrics . [source] AdaBoostClassifier nnetsauce.AdaBoostClassifier( obj, n_estimators=10, learning_rate=0.1, n_hidden_features=1, reg_lambda=0, reg_alpha=0.5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=False, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, verbose=1, method=\"SAMME\", backend=\"cpu\", ) AdaBoost Classification (SAMME) model class derived from class Boosting Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_estimators: int number of boosting iterations learning_rate: float learning rate of the boosting procedure n_hidden_features: int number of nodes in the hidden layer reg_lambda: float regularization parameter for weights reg_alpha: float controls compromize between l1 and l2 norm of weights activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' method: str type of Adaboost method, 'SAMME' (discrete) or 'SAMME.R' (real) backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: alpha_: list AdaBoost coefficients alpha_m base_learners_: dict a dictionary containing the base learners Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/adaboost_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) # SAMME.R clf = LogisticRegression(solver='liblinear', multi_class = 'ovr', random_state=123) fit_obj = ns.AdaBoostClassifier(clf, n_hidden_features=int(11.22338867), direct_link=True, n_estimators=250, learning_rate=0.01126343, col_sample=0.72684326, row_sample=0.86429443, dropout=0.63078613, n_clusters=2, type_clust=\"gmm\", verbose=1, seed = 123, method=\"SAMME.R\") start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") start = time() print(fit_obj.score(X_test, y_test)) print(f\"Elapsed {time() - start}\") preds = fit_obj.predict(X_test) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) print(metrics.classification_report(preds, y_test)) [source] fit AdaBoostClassifier.fit(X, y, sample_weight=None, **kwargs) Fit Boosting model to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] predict AdaBoostClassifier.predict(X, **kwargs) Predict test data X. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: model predictions: {array-like} [source] predict_proba AdaBoostClassifier.predict_proba(X, **kwargs) Predict probabilities for test data X. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source] score AdaBoostClassifier.score(X, y, scoring=None, **kwargs) Score the model on test set features X and response y. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values scoring: str must be in ('accuracy', 'average_precision', 'brier_score_loss', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'neg_log_loss', 'precision', 'recall', 'roc_auc') **kwargs: additional parameters to be passed to scoring functions Returns: model scores: {array-like} [source] CustomClassifier nnetsauce.CustomClassifier( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Custom Classification model Attributes: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Examples: import nnetsauce as ns from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.datasets import load_digits from time import time digits = load_digits() X = digits.data y = digits.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) # layer 1 (base layer) ---- layer1_regr = RandomForestClassifier(n_estimators=10, random_state=123) start = time() layer1_regr.fit(X_train, y_train) # Accuracy in layer 1 print(layer1_regr.score(X_test, y_test)) # layer 2 using layer 1 ---- layer2_regr = ns.CustomClassifier(obj = layer1_regr, n_hidden_features=5, direct_link=True, bias=True, nodes_sim='uniform', activation_name='relu', n_clusters=2, seed=123) layer2_regr.fit(X_train, y_train) # Accuracy in layer 2 print(layer2_regr.score(X_test, y_test)) # layer 3 using layer 2 ---- layer3_regr = ns.CustomClassifier(obj = layer2_regr, n_hidden_features=10, direct_link=True, bias=True, dropout=0.7, nodes_sim='uniform', activation_name='relu', n_clusters=2, seed=123) layer3_regr.fit(X_train, y_train) # Accuracy in layer 3 print(layer3_regr.score(X_test, y_test)) print(f\"Elapsed {time() - start}\") [source] fit CustomClassifier.fit(X, y, sample_weight=None, **kwargs) Fit custom model to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] GLMClassifier nnetsauce.GLMClassifier( n_hidden_features=5, lambda1=0.01, alpha1=0.5, lambda2=0.01, alpha2=0.5, family=\"expit\", activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), optimizer=Optimizer(), seed=123, ) Generalized 'linear' models using quasi-randomized networks (classification) Parameters: n_hidden_features: int number of nodes in the hidden layer lambda1: float regularization parameter for GLM coefficients on original features alpha1: float controls compromize between l1 and l2 norm of GLM coefficients on original features lambda2: float regularization parameter for GLM coefficients on nonlinear features alpha2: float controls compromize between l1 and l2 norm of GLM coefficients on nonlinear features activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') optimizer: object optimizer, from class nnetsauce.utils.Optimizer seed: int reproducibility seed for nodes_sim=='uniform' Attributes: beta_: vector regression coefficients Examples: See https://github.com/Techtonique/nnetsauce/blob/master/examples/glm_classification.py [source] fit GLMClassifier.fit( X, y, learning_rate=0.01, decay=0.1, batch_prop=1, tolerance=1e-05, optimizer=None, verbose=1, **kwargs ) Fit GLM model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] MultitaskClassifier nnetsauce.MultitaskClassifier( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Multitask Classification model based on regression models, with shared covariates Parameters: obj: object any object (must be a regression model) containing a method fit (obj.fit()) and a method predict (obj.predict()) n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: fit_objs_: dict objects adjusted to each individual time series n_classes_: int number of classes for the classifier Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/mtask_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2, random_state=123+2*10) # Linear Regression is used regr = LinearRegression() fit_obj = ns.MultitaskClassifier(regr, n_hidden_features=5, n_clusters=2, type_clust=\"gmm\") start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(f\"Elapsed {time() - start}\") print(metrics.classification_report(preds, y_test)) [source] fit MultitaskClassifier.fit(X, y, sample_weight=None, **kwargs) Fit MultitaskClassifier to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] RandomBagClassifier nnetsauce.RandomBagClassifier( obj, n_estimators=10, n_hidden_features=1, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=False, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, n_jobs=None, seed=123, verbose=1, backend=\"cpu\", ) Randomized 'Bagging' Classification model Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_estimators: int number of boosting iterations n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: voter_: dict dictionary containing all the fitted base-learners Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/randombag_classification.py import nnetsauce as ns from sklearn.datasets import load_breast_cancer from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) # decision tree clf = DecisionTreeClassifier(max_depth=2, random_state=123) fit_obj = ns.RandomBagClassifier(clf, n_hidden_features=2, direct_link=True, n_estimators=100, col_sample=0.9, row_sample=0.9, dropout=0.3, n_clusters=0, verbose=1) start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(f\"Elapsed {time() - start}\") print(metrics.classification_report(preds, y_test)) [source] fit RandomBagClassifier.fit(X, y, **kwargs) Fit Random 'Bagging' model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] Ridge2Classifier nnetsauce.Ridge2Classifier( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lambda1=0.1, lambda2=0.1, seed=123, backend=\"cpu\", ) Multinomial logit classification with 2 regularization parameters Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') lambda1: float regularization parameter on direct link lambda2: float regularization parameter on hidden layer seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: {array-like} regression coefficients Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/ridge_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from time import time breast_cancer = load_breast_cancer() X = breast_cancer.data y = breast_cancer.target # split data into training test and test set np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # create the model with nnetsauce fit_obj = ns.Ridge2Classifier(lambda1 = 6.90185578e+04, lambda2 = 3.17392781e+02, n_hidden_features=95, n_clusters=2, dropout = 3.62817383e-01, type_clust = \"gmm\") # fit the model on training set start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") # get the accuracy on test set start = time() print(fit_obj.score(X_test, y_test)) print(f\"Elapsed {time() - start}\") # get area under the curve on test set (auc) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) [source] fit Ridge2Classifier.fit(X, y, solver=\"L-BFGS-B\", **kwargs) Fit Ridge model to training data (X, y). for beta: regression coeffs (beta11, ..., beta1p, ..., betaK1, ..., betaKp) for K classes and p covariates. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] Ridge2MultitaskClassifier nnetsauce.Ridge2MultitaskClassifier( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lambda1=0.1, lambda2=0.1, seed=123, backend=\"cpu\", ) Multitask Ridge classification with 2 regularization parameters Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') lambda1: float regularization parameter on direct link lambda2: float regularization parameter on hidden layer seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: {array-like} regression coefficients Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/ridgemtask_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) fit_obj = ns.Ridge2MultitaskClassifier(n_hidden_features=int(9.83730469e+01), dropout=4.31054687e-01, n_clusters=int(1.71484375e+00), lambda1=1.24023438e+01, lambda2=7.30263672e+03) start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(f\"Elapsed {time() - start}\") print(metrics.classification_report(preds, y_test)) [source] fit Ridge2MultitaskClassifier.fit(X, y, **kwargs) Fit Ridge model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object","title":"Classifiers"},{"location":"documentation/classifiers/#classifiers","text":"In alphabetical order All models possess methods fit , predict , predict_proba , and score . For scoring metrics, refer to scoring metrics . [source]","title":"Classifiers"},{"location":"documentation/classifiers/#adaboostclassifier","text":"nnetsauce.AdaBoostClassifier( obj, n_estimators=10, learning_rate=0.1, n_hidden_features=1, reg_lambda=0, reg_alpha=0.5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=False, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, verbose=1, method=\"SAMME\", backend=\"cpu\", ) AdaBoost Classification (SAMME) model class derived from class Boosting Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_estimators: int number of boosting iterations learning_rate: float learning rate of the boosting procedure n_hidden_features: int number of nodes in the hidden layer reg_lambda: float regularization parameter for weights reg_alpha: float controls compromize between l1 and l2 norm of weights activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' method: str type of Adaboost method, 'SAMME' (discrete) or 'SAMME.R' (real) backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: alpha_: list AdaBoost coefficients alpha_m base_learners_: dict a dictionary containing the base learners Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/adaboost_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) # SAMME.R clf = LogisticRegression(solver='liblinear', multi_class = 'ovr', random_state=123) fit_obj = ns.AdaBoostClassifier(clf, n_hidden_features=int(11.22338867), direct_link=True, n_estimators=250, learning_rate=0.01126343, col_sample=0.72684326, row_sample=0.86429443, dropout=0.63078613, n_clusters=2, type_clust=\"gmm\", verbose=1, seed = 123, method=\"SAMME.R\") start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") start = time() print(fit_obj.score(X_test, y_test)) print(f\"Elapsed {time() - start}\") preds = fit_obj.predict(X_test) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) print(metrics.classification_report(preds, y_test)) [source]","title":"AdaBoostClassifier"},{"location":"documentation/classifiers/#fit","text":"AdaBoostClassifier.fit(X, y, sample_weight=None, **kwargs) Fit Boosting model to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/classifiers/#predict","text":"AdaBoostClassifier.predict(X, **kwargs) Predict test data X. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: model predictions: {array-like} [source]","title":"predict"},{"location":"documentation/classifiers/#predict_proba","text":"AdaBoostClassifier.predict_proba(X, **kwargs) Predict probabilities for test data X. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. **kwargs: additional parameters to be passed to self.cook_test_set Returns: probability estimates for test data: {array-like} [source]","title":"predict_proba"},{"location":"documentation/classifiers/#score","text":"AdaBoostClassifier.score(X, y, scoring=None, **kwargs) Score the model on test set features X and response y. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values scoring: str must be in ('accuracy', 'average_precision', 'brier_score_loss', 'f1', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'neg_log_loss', 'precision', 'recall', 'roc_auc') **kwargs: additional parameters to be passed to scoring functions Returns: model scores: {array-like} [source]","title":"score"},{"location":"documentation/classifiers/#customclassifier","text":"nnetsauce.CustomClassifier( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Custom Classification model Attributes: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Examples: import nnetsauce as ns from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.datasets import load_digits from time import time digits = load_digits() X = digits.data y = digits.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) # layer 1 (base layer) ---- layer1_regr = RandomForestClassifier(n_estimators=10, random_state=123) start = time() layer1_regr.fit(X_train, y_train) # Accuracy in layer 1 print(layer1_regr.score(X_test, y_test)) # layer 2 using layer 1 ---- layer2_regr = ns.CustomClassifier(obj = layer1_regr, n_hidden_features=5, direct_link=True, bias=True, nodes_sim='uniform', activation_name='relu', n_clusters=2, seed=123) layer2_regr.fit(X_train, y_train) # Accuracy in layer 2 print(layer2_regr.score(X_test, y_test)) # layer 3 using layer 2 ---- layer3_regr = ns.CustomClassifier(obj = layer2_regr, n_hidden_features=10, direct_link=True, bias=True, dropout=0.7, nodes_sim='uniform', activation_name='relu', n_clusters=2, seed=123) layer3_regr.fit(X_train, y_train) # Accuracy in layer 3 print(layer3_regr.score(X_test, y_test)) print(f\"Elapsed {time() - start}\") [source]","title":"CustomClassifier"},{"location":"documentation/classifiers/#fit_1","text":"CustomClassifier.fit(X, y, sample_weight=None, **kwargs) Fit custom model to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/classifiers/#glmclassifier","text":"nnetsauce.GLMClassifier( n_hidden_features=5, lambda1=0.01, alpha1=0.5, lambda2=0.01, alpha2=0.5, family=\"expit\", activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), optimizer=Optimizer(), seed=123, ) Generalized 'linear' models using quasi-randomized networks (classification) Parameters: n_hidden_features: int number of nodes in the hidden layer lambda1: float regularization parameter for GLM coefficients on original features alpha1: float controls compromize between l1 and l2 norm of GLM coefficients on original features lambda2: float regularization parameter for GLM coefficients on nonlinear features alpha2: float controls compromize between l1 and l2 norm of GLM coefficients on nonlinear features activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') optimizer: object optimizer, from class nnetsauce.utils.Optimizer seed: int reproducibility seed for nodes_sim=='uniform' Attributes: beta_: vector regression coefficients Examples: See https://github.com/Techtonique/nnetsauce/blob/master/examples/glm_classification.py [source]","title":"GLMClassifier"},{"location":"documentation/classifiers/#fit_2","text":"GLMClassifier.fit( X, y, learning_rate=0.01, decay=0.1, batch_prop=1, tolerance=1e-05, optimizer=None, verbose=1, **kwargs ) Fit GLM model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/classifiers/#multitaskclassifier","text":"nnetsauce.MultitaskClassifier( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Multitask Classification model based on regression models, with shared covariates Parameters: obj: object any object (must be a regression model) containing a method fit (obj.fit()) and a method predict (obj.predict()) n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: fit_objs_: dict objects adjusted to each individual time series n_classes_: int number of classes for the classifier Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/mtask_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2, random_state=123+2*10) # Linear Regression is used regr = LinearRegression() fit_obj = ns.MultitaskClassifier(regr, n_hidden_features=5, n_clusters=2, type_clust=\"gmm\") start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(f\"Elapsed {time() - start}\") print(metrics.classification_report(preds, y_test)) [source]","title":"MultitaskClassifier"},{"location":"documentation/classifiers/#fit_3","text":"MultitaskClassifier.fit(X, y, sample_weight=None, **kwargs) Fit MultitaskClassifier to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/classifiers/#randombagclassifier","text":"nnetsauce.RandomBagClassifier( obj, n_estimators=10, n_hidden_features=1, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=False, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, n_jobs=None, seed=123, verbose=1, backend=\"cpu\", ) Randomized 'Bagging' Classification model Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_estimators: int number of boosting iterations n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: voter_: dict dictionary containing all the fitted base-learners Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/randombag_classification.py import nnetsauce as ns from sklearn.datasets import load_breast_cancer from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) # decision tree clf = DecisionTreeClassifier(max_depth=2, random_state=123) fit_obj = ns.RandomBagClassifier(clf, n_hidden_features=2, direct_link=True, n_estimators=100, col_sample=0.9, row_sample=0.9, dropout=0.3, n_clusters=0, verbose=1) start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(f\"Elapsed {time() - start}\") print(metrics.classification_report(preds, y_test)) [source]","title":"RandomBagClassifier"},{"location":"documentation/classifiers/#fit_4","text":"RandomBagClassifier.fit(X, y, **kwargs) Fit Random 'Bagging' model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/classifiers/#ridge2classifier","text":"nnetsauce.Ridge2Classifier( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lambda1=0.1, lambda2=0.1, seed=123, backend=\"cpu\", ) Multinomial logit classification with 2 regularization parameters Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') lambda1: float regularization parameter on direct link lambda2: float regularization parameter on hidden layer seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: {array-like} regression coefficients Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/ridge_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from time import time breast_cancer = load_breast_cancer() X = breast_cancer.data y = breast_cancer.target # split data into training test and test set np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # create the model with nnetsauce fit_obj = ns.Ridge2Classifier(lambda1 = 6.90185578e+04, lambda2 = 3.17392781e+02, n_hidden_features=95, n_clusters=2, dropout = 3.62817383e-01, type_clust = \"gmm\") # fit the model on training set start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") # get the accuracy on test set start = time() print(fit_obj.score(X_test, y_test)) print(f\"Elapsed {time() - start}\") # get area under the curve on test set (auc) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) [source]","title":"Ridge2Classifier"},{"location":"documentation/classifiers/#fit_5","text":"Ridge2Classifier.fit(X, y, solver=\"L-BFGS-B\", **kwargs) Fit Ridge model to training data (X, y). for beta: regression coeffs (beta11, ..., beta1p, ..., betaK1, ..., betaKp) for K classes and p covariates. Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/classifiers/#ridge2multitaskclassifier","text":"nnetsauce.Ridge2MultitaskClassifier( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lambda1=0.1, lambda2=0.1, seed=123, backend=\"cpu\", ) Multitask Ridge classification with 2 regularization parameters Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') lambda1: float regularization parameter on direct link lambda2: float regularization parameter on hidden layer seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: {array-like} regression coefficients Examples: See also https://github.com/Techtonique/nnetsauce/blob/master/examples/ridgemtask_classification.py import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn import metrics from time import time breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) fit_obj = ns.Ridge2MultitaskClassifier(n_hidden_features=int(9.83730469e+01), dropout=4.31054687e-01, n_clusters=int(1.71484375e+00), lambda1=1.24023438e+01, lambda2=7.30263672e+03) start = time() fit_obj.fit(X_train, y_train) print(f\"Elapsed {time() - start}\") print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(f\"Elapsed {time() - start}\") print(metrics.classification_report(preds, y_test)) [source]","title":"Ridge2MultitaskClassifier"},{"location":"documentation/classifiers/#fit_6","text":"Ridge2MultitaskClassifier.fit(X, y, **kwargs) Fit Ridge model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object","title":"fit"},{"location":"documentation/regressors/","text":"Regressors In alphabetical order All models possess methods fit , predict , and score . Methods predict and score are only documented for the first model; the same principles apply subsequently . For scoring metrics, refer to scoring metrics . [source] BaseRegressor nnetsauce.BaseRegressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Random Vector Functional Link Network regression without shrinkage Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for hidden layer nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original features are included (True) in model's fitting or not (False) n_clusters: int number of clusters for type_clust='kmeans' or type_clust='gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot); if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of features randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: vector regression coefficients GCV_: float Generalized Cross-Validation error [source] fit BaseRegressor.fit(X, y, **kwargs) Fit BaseRegressor to training data (X, y) Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source] predict BaseRegressor.predict(X, **kwargs) Predict test data X. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features **kwargs: additional parameters to be passed to self.cook_test_set Returns: model predictions: {array-like} [source] score BaseRegressor.score(X, y, scoring=None, **kwargs) Score the model on test set features X and response y. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values scoring: str must be in ('explained_variance', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'r2') **kwargs: additional parameters to be passed to scoring functions Returns: model scores: {array-like} [source] BayesianRVFLRegressor nnetsauce.BayesianRVFLRegressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), seed=123, s=0.1, sigma=0.05, return_std=True, backend=\"cpu\", ) Bayesian Random Vector Functional Link Network regression with one prior Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original features are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') seed: int reproducibility seed for nodes_sim=='uniform' s: float std. dev. of regression parameters in Bayesian Ridge Regression sigma: float std. dev. of residuals in Bayesian Ridge Regression return_std: boolean if True, uncertainty around predictions is evaluated backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: array-like regression''s coefficients Sigma_: array-like covariance of the distribution of fitted parameters GCV_: float Generalized cross-validation error y_mean_: float average response Examples: TBD [source] fit BayesianRVFLRegressor.fit(X, y, **kwargs) Fit BayesianRVFLRegressor to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source] BayesianRVFL2Regressor nnetsauce.BayesianRVFL2Regressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=0, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), seed=123, s1=0.1, s2=0.1, sigma=0.05, return_std=True, backend=\"cpu\", ) Bayesian Random Vector Functional Link Network regression with two priors Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original features are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') seed: int reproducibility seed for nodes_sim=='uniform' s1: float std. dev. of init. regression parameters in Bayesian Ridge Regression s2: float std. dev. of augmented regression parameters in Bayesian Ridge Regression sigma: float std. dev. of residuals in Bayesian Ridge Regression return_std: boolean if True, uncertainty around predictions is evaluated backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: array-like regression''s coefficients Sigma_: array-like covariance of the distribution of fitted parameters GCV_: float Generalized cross-validation error y_mean_: float average response Examples: TBD [source] fit BayesianRVFL2Regressor.fit(X, y, **kwargs) Fit BayesianRVFL2Regressor to training data (X, y) Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source] CustomRegressor nnetsauce.CustomRegressor( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Custom Regression model This class is used to 'augment' any regression model with transformed features. Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' type_fit: str 'regression' backend: str \"cpu\" or \"gpu\" or \"tpu\" Examples: TBD [source] fit CustomRegressor.fit(X, y, sample_weight=None, **kwargs) Fit custom model to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] GLMRegressor nnetsauce.GLMRegressor( n_hidden_features=5, lambda1=0.01, alpha1=0.5, lambda2=0.01, alpha2=0.5, family=\"gaussian\", activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), optimizer=Optimizer(), seed=123, ) Generalized 'linear' models using quasi-randomized networks (regression) Attributes: n_hidden_features: int number of nodes in the hidden layer lambda1: float regularization parameter for GLM coefficients on original features alpha1: float controls compromize between l1 and l2 norm of GLM coefficients on original features lambda2: float regularization parameter for GLM coefficients on nonlinear features alpha2: float controls compromize between l1 and l2 norm of GLM coefficients on nonlinear features family: str \"gaussian\", \"laplace\" or \"poisson\" (for now) activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') optimizer: object optimizer, from class nnetsauce.utils.Optimizer seed: int reproducibility seed for nodes_sim=='uniform' Attributes: beta_: vector regression coefficients Examples: See https://github.com/Techtonique/nnetsauce/blob/master/examples/glm_regression.py [source] fit GLMRegressor.fit( X, y, learning_rate=0.01, decay=0.1, batch_prop=1, tolerance=1e-05, optimizer=None, verbose=0, **kwargs ) Fit GLM model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] RandomBagRegressor nnetsauce.RandomBagRegressor( obj, n_estimators=10, n_hidden_features=1, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=False, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, n_jobs=None, seed=123, verbose=1, backend=\"cpu\", ) Randomized 'Bagging' Regression model Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_estimators: int number of boosting iterations n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: voter_: dict dictionary containing all the fitted base-learners Examples: import numpy as np import nnetsauce as ns from sklearn.datasets import fetch_california_housing from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import train_test_split X, y = fetch_california_housing(return_X_y=True, as_frame=False) # split data into training test and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13) # Requires further tuning obj = DecisionTreeRegressor(max_depth=3, random_state=123) obj2 = ns.RandomBagRegressor(obj=obj, direct_link=False, n_estimators=50, col_sample=0.9, row_sample=0.9, dropout=0, n_clusters=0, verbose=1) obj2.fit(X_train, y_train) print(np.sqrt(obj2.score(X_test, y_test))) # RMSE [source] fit RandomBagRegressor.fit(X, y, **kwargs) Fit Random 'Bagging' model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source] Ridge2Regressor nnetsauce.Ridge2Regressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lambda1=0.1, lambda2=0.1, seed=123, backend=\"cpu\", ) Ridge regression with 2 regularization parameters derived from class Ridge Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') lambda1: float regularization parameter on direct link lambda2: float regularization parameter on hidden layer seed: int reproducibility seed for nodes_sim=='uniform' backend: str 'cpu' or 'gpu' or 'tpu' Attributes: beta_: {array-like} regression coefficients y_mean_: float average response [source] fit Ridge2Regressor.fit(X, y, **kwargs) Fit Ridge model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object","title":"Regressors"},{"location":"documentation/regressors/#regressors","text":"In alphabetical order All models possess methods fit , predict , and score . Methods predict and score are only documented for the first model; the same principles apply subsequently . For scoring metrics, refer to scoring metrics . [source]","title":"Regressors"},{"location":"documentation/regressors/#baseregressor","text":"nnetsauce.BaseRegressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Random Vector Functional Link Network regression without shrinkage Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for hidden layer nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original features are included (True) in model's fitting or not (False) n_clusters: int number of clusters for type_clust='kmeans' or type_clust='gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot); if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of features randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform', clustering and dropout backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: vector regression coefficients GCV_: float Generalized Cross-Validation error [source]","title":"BaseRegressor"},{"location":"documentation/regressors/#fit","text":"BaseRegressor.fit(X, y, **kwargs) Fit BaseRegressor to training data (X, y) Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source]","title":"fit"},{"location":"documentation/regressors/#predict","text":"BaseRegressor.predict(X, **kwargs) Predict test data X. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features **kwargs: additional parameters to be passed to self.cook_test_set Returns: model predictions: {array-like} [source]","title":"predict"},{"location":"documentation/regressors/#score","text":"BaseRegressor.score(X, y, scoring=None, **kwargs) Score the model on test set features X and response y. Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values scoring: str must be in ('explained_variance', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'r2') **kwargs: additional parameters to be passed to scoring functions Returns: model scores: {array-like} [source]","title":"score"},{"location":"documentation/regressors/#bayesianrvflregressor","text":"nnetsauce.BayesianRVFLRegressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), seed=123, s=0.1, sigma=0.05, return_std=True, backend=\"cpu\", ) Bayesian Random Vector Functional Link Network regression with one prior Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original features are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') seed: int reproducibility seed for nodes_sim=='uniform' s: float std. dev. of regression parameters in Bayesian Ridge Regression sigma: float std. dev. of residuals in Bayesian Ridge Regression return_std: boolean if True, uncertainty around predictions is evaluated backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: array-like regression''s coefficients Sigma_: array-like covariance of the distribution of fitted parameters GCV_: float Generalized cross-validation error y_mean_: float average response Examples: TBD [source]","title":"BayesianRVFLRegressor"},{"location":"documentation/regressors/#fit_1","text":"BayesianRVFLRegressor.fit(X, y, **kwargs) Fit BayesianRVFLRegressor to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source]","title":"fit"},{"location":"documentation/regressors/#bayesianrvfl2regressor","text":"nnetsauce.BayesianRVFL2Regressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=0, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), seed=123, s1=0.1, s2=0.1, sigma=0.05, return_std=True, backend=\"cpu\", ) Bayesian Random Vector Functional Link Network regression with two priors Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original features are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') seed: int reproducibility seed for nodes_sim=='uniform' s1: float std. dev. of init. regression parameters in Bayesian Ridge Regression s2: float std. dev. of augmented regression parameters in Bayesian Ridge Regression sigma: float std. dev. of residuals in Bayesian Ridge Regression return_std: boolean if True, uncertainty around predictions is evaluated backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: beta_: array-like regression''s coefficients Sigma_: array-like covariance of the distribution of fitted parameters GCV_: float Generalized cross-validation error y_mean_: float average response Examples: TBD [source]","title":"BayesianRVFL2Regressor"},{"location":"documentation/regressors/#fit_2","text":"BayesianRVFL2Regressor.fit(X, y, **kwargs) Fit BayesianRVFL2Regressor to training data (X, y) Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features y: array-like, shape = [n_samples] Target values **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source]","title":"fit"},{"location":"documentation/regressors/#customregressor","text":"nnetsauce.CustomRegressor( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, seed=123, backend=\"cpu\", ) Custom Regression model This class is used to 'augment' any regression model with transformed features. Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' type_fit: str 'regression' backend: str \"cpu\" or \"gpu\" or \"tpu\" Examples: TBD [source]","title":"CustomRegressor"},{"location":"documentation/regressors/#fit_3","text":"CustomRegressor.fit(X, y, sample_weight=None, **kwargs) Fit custom model to training data (X, y). Parameters: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/regressors/#glmregressor","text":"nnetsauce.GLMRegressor( n_hidden_features=5, lambda1=0.01, alpha1=0.5, lambda2=0.01, alpha2=0.5, family=\"gaussian\", activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), optimizer=Optimizer(), seed=123, ) Generalized 'linear' models using quasi-randomized networks (regression) Attributes: n_hidden_features: int number of nodes in the hidden layer lambda1: float regularization parameter for GLM coefficients on original features alpha1: float controls compromize between l1 and l2 norm of GLM coefficients on original features lambda2: float regularization parameter for GLM coefficients on nonlinear features alpha2: float controls compromize between l1 and l2 norm of GLM coefficients on nonlinear features family: str \"gaussian\", \"laplace\" or \"poisson\" (for now) activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model's fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') optimizer: object optimizer, from class nnetsauce.utils.Optimizer seed: int reproducibility seed for nodes_sim=='uniform' Attributes: beta_: vector regression coefficients Examples: See https://github.com/Techtonique/nnetsauce/blob/master/examples/glm_regression.py [source]","title":"GLMRegressor"},{"location":"documentation/regressors/#fit_4","text":"GLMRegressor.fit( X, y, learning_rate=0.01, decay=0.1, batch_prop=1, tolerance=1e-05, optimizer=None, verbose=0, **kwargs ) Fit GLM model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/regressors/#randombagregressor","text":"nnetsauce.RandomBagRegressor( obj, n_estimators=10, n_hidden_features=1, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=False, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), col_sample=1, row_sample=1, n_jobs=None, seed=123, verbose=1, backend=\"cpu\", ) Randomized 'Bagging' Regression model Parameters: obj: object any object containing a method fit (obj.fit()) and a method predict (obj.predict()) n_estimators: int number of boosting iterations n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training direct_link: boolean indicates if the original predictors are included (True) in model''s fitting or not (False) n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') col_sample: float percentage of covariates randomly chosen for training row_sample: float percentage of rows chosen for training, by stratified bootstrapping seed: int reproducibility seed for nodes_sim=='uniform' backend: str \"cpu\" or \"gpu\" or \"tpu\" Attributes: voter_: dict dictionary containing all the fitted base-learners Examples: import numpy as np import nnetsauce as ns from sklearn.datasets import fetch_california_housing from sklearn.tree import DecisionTreeRegressor from sklearn.model_selection import train_test_split X, y = fetch_california_housing(return_X_y=True, as_frame=False) # split data into training test and test set X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13) # Requires further tuning obj = DecisionTreeRegressor(max_depth=3, random_state=123) obj2 = ns.RandomBagRegressor(obj=obj, direct_link=False, n_estimators=50, col_sample=0.9, row_sample=0.9, dropout=0, n_clusters=0, verbose=1) obj2.fit(X_train, y_train) print(np.sqrt(obj2.score(X_test, y_test))) # RMSE [source]","title":"RandomBagRegressor"},{"location":"documentation/regressors/#fit_5","text":"RandomBagRegressor.fit(X, y, **kwargs) Fit Random 'Bagging' model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object [source]","title":"fit"},{"location":"documentation/regressors/#ridge2regressor","text":"nnetsauce.Ridge2Regressor( n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lambda1=0.1, lambda2=0.1, seed=123, backend=\"cpu\", ) Ridge regression with 2 regularization parameters derived from class Ridge Parameters: n_hidden_features: int number of nodes in the hidden layer activation_name: str activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu' a: float hyperparameter for 'prelu' or 'elu' activation function nodes_sim: str type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform' bias: boolean indicates if the hidden layer contains a bias term (True) or not (False) dropout: float regularization parameter; (random) percentage of nodes dropped out of the training n_clusters: int number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering) cluster_encode: bool defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding type_clust: str type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm') type_scaling: a tuple of 3 strings scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax') lambda1: float regularization parameter on direct link lambda2: float regularization parameter on hidden layer seed: int reproducibility seed for nodes_sim=='uniform' backend: str 'cpu' or 'gpu' or 'tpu' Attributes: beta_: {array-like} regression coefficients y_mean_: float average response [source]","title":"Ridge2Regressor"},{"location":"documentation/regressors/#fit_6","text":"Ridge2Regressor.fit(X, y, **kwargs) Fit Ridge model to training data (X, y). Args: X: {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y: array-like, shape = [n_samples] Target values. **kwargs: additional parameters to be passed to self.cook_training_set or self.obj.fit Returns: self: object","title":"fit"},{"location":"documentation/scoring_metrics/","text":"Scoring metrics Regression metrics explained_variance : Explained variance regression score function mean_absolute_error : Mean absolute error regression loss mean_squared_error : Mean squared error regression loss mean_squared_log_error : Mean squared logarithmic error regression loss median_absolute_error : Median absolute error regression loss r2 : R^2 (coefficient of determination) regression score function. Classification metrics accuracy : Accuracy classification score. average_precision : Compute average precision (AP) from prediction scores brier_score_loss : Compute the Brier score. f1_score : Compute the F1 score, also known as balanced F-score or F-measure neg_log_loss : Negative Log loss, aka logistic loss or cross-entropy loss. precision : Compute the precision recall : Compute the recall roc_auc : Compute Area Under the Curve (AUC) using the trapezoidal rule","title":"Scoring metrics"},{"location":"documentation/scoring_metrics/#scoring-metrics","text":"","title":"Scoring metrics"},{"location":"documentation/scoring_metrics/#regression-metrics","text":"explained_variance : Explained variance regression score function mean_absolute_error : Mean absolute error regression loss mean_squared_error : Mean squared error regression loss mean_squared_log_error : Mean squared logarithmic error regression loss median_absolute_error : Median absolute error regression loss r2 : R^2 (coefficient of determination) regression score function.","title":"Regression metrics"},{"location":"documentation/scoring_metrics/#classification-metrics","text":"accuracy : Accuracy classification score. average_precision : Compute average precision (AP) from prediction scores brier_score_loss : Compute the Brier score. f1_score : Compute the F1 score, also known as balanced F-score or F-measure neg_log_loss : Negative Log loss, aka logistic loss or cross-entropy loss. precision : Compute the precision recall : Compute the recall roc_auc : Compute Area Under the Curve (AUC) using the trapezoidal rule","title":"Classification metrics"},{"location":"documentation/time_series/","text":"Time series models In alphabetical order All models possess methods: fit , predict . [source] MTS nnetsauce.MTS( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lags=1, seed=123, backend=\"cpu\", ) Univariate and multivariate time series (MTS) forecasting with Quasi-Randomized networks Parameters: obj: object. any object containing a method fit (obj.fit()) and a method predict (obj.predict()). n_hidden_features: int. number of nodes in the hidden layer. activation_name: str. activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'. a: float. hyperparameter for 'prelu' or 'elu' activation function. nodes_sim: str. type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform'. bias: boolean. indicates if the hidden layer contains a bias term (True) or not (False). dropout: float. regularization parameter; (random) percentage of nodes dropped out of the training. direct_link: boolean. indicates if the original predictors are included (True) in model's fitting or not (False). n_clusters: int. number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering). cluster_encode: bool. defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding. type_clust: str. type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm'). type_scaling: a tuple of 3 strings. scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax'). lags: int. number of lags used for each time series. seed: int. reproducibility seed for nodes_sim=='uniform'. backend: str. \"cpu\" or \"gpu\" or \"tpu\". Attributes: fit_objs_: dict objects adjusted to each individual time series y_: {array-like} MTS responses (most recent observations first) X_: {array-like} MTS lags xreg_: {array-like} external regressors y_means_: dict a dictionary of each series mean values preds_: {array-like} successive model predictions preds_std_: {array-like} standard deviation around the predictions return_std_: boolean return uncertainty or not (set in predict) df_: data frame the input data frame, in case a data.frame is provided to `fit` Examples: Example 1: import nnetsauce as ns import numpy as np from sklearn import linear_model np.random.seed(123) M = np.random.rand(10, 3) M[:,0] = 10*M[:,0] M[:,2] = 25*M[:,2] print(M) # Adjust Bayesian Ridge regr4 = linear_model.BayesianRidge() obj_MTS = ns.MTS(regr4, lags = 1, n_hidden_features=5) obj_MTS.fit(M) print(obj_MTS.predict()) # with credible intervals print(obj_MTS.predict(return_std=True, level=80)) print(obj_MTS.predict(return_std=True, level=95)) Example 2: import nnetsauce as ns import numpy as np from sklearn import linear_model dataset = { 'date' : ['2001-01-01', '2002-01-01', '2003-01-01', '2004-01-01', '2005-01-01'], 'series1' : [34, 30, 35.6, 33.3, 38.1], 'series2' : [4, 5.5, 5.6, 6.3, 5.1], 'series3' : [100, 100.5, 100.6, 100.2, 100.1]} df = pd.DataFrame(dataset).set_index('date') print(df) # Adjust Bayesian Ridge regr5 = linear_model.BayesianRidge() obj_MTS = ns.MTS(regr5, lags = 1, n_hidden_features=5) obj_MTS.fit(df) print(obj_MTS.predict()) # with credible intervals print(obj_MTS.predict(return_std=True, level=80)) print(obj_MTS.predict(return_std=True, level=95)) [source] fit MTS.fit(X, xreg=None) Fit MTS model to training data X, with optional regressors xreg Parameters: X: {array-like}, shape = [n_samples, n_features] Training time series, where n_samples is the number of samples and n_features is the number of features; X must be in increasing order (most recent observations last) xreg: {array-like}, shape = [n_samples, n_features_xreg] Additional regressors to be passed to obj xreg must be in increasing order (most recent observations last) **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source] predict MTS.predict(h=5, level=95, new_xreg=None, **kwargs) Forecast all the time series, h steps ahead Parameters: h: {integer} Forecasting horizon level: {integer} Level of confidence (if obj has option 'return_std' and the posterior is gaussian) new_xreg: {array-like}, shape = [n_samples = h, n_new_xreg] New values of additional (deterministic) regressors on horizon = h new_xreg must be in increasing order (most recent observations last) **kwargs: additional parameters to be passed to self.cook_test_set Returns: model predictions for horizon = h: {array-like}, data frame or tuple. Standard deviation and prediction intervals are returned when `obj.predict` can return standard deviation","title":"Time series"},{"location":"documentation/time_series/#time-series-models","text":"In alphabetical order All models possess methods: fit , predict . [source]","title":"Time series models"},{"location":"documentation/time_series/#mts","text":"nnetsauce.MTS( obj, n_hidden_features=5, activation_name=\"relu\", a=0.01, nodes_sim=\"sobol\", bias=True, dropout=0, direct_link=True, n_clusters=2, cluster_encode=True, type_clust=\"kmeans\", type_scaling=(\"std\", \"std\", \"std\"), lags=1, seed=123, backend=\"cpu\", ) Univariate and multivariate time series (MTS) forecasting with Quasi-Randomized networks Parameters: obj: object. any object containing a method fit (obj.fit()) and a method predict (obj.predict()). n_hidden_features: int. number of nodes in the hidden layer. activation_name: str. activation function: 'relu', 'tanh', 'sigmoid', 'prelu' or 'elu'. a: float. hyperparameter for 'prelu' or 'elu' activation function. nodes_sim: str. type of simulation for the nodes: 'sobol', 'hammersley', 'halton', 'uniform'. bias: boolean. indicates if the hidden layer contains a bias term (True) or not (False). dropout: float. regularization parameter; (random) percentage of nodes dropped out of the training. direct_link: boolean. indicates if the original predictors are included (True) in model's fitting or not (False). n_clusters: int. number of clusters for 'kmeans' or 'gmm' clustering (could be 0: no clustering). cluster_encode: bool. defines how the variable containing clusters is treated (default is one-hot) if `False`, then labels are used, without one-hot encoding. type_clust: str. type of clustering method: currently k-means ('kmeans') or Gaussian Mixture Model ('gmm'). type_scaling: a tuple of 3 strings. scaling methods for inputs, hidden layer, and clustering respectively (and when relevant). Currently available: standardization ('std') or MinMax scaling ('minmax'). lags: int. number of lags used for each time series. seed: int. reproducibility seed for nodes_sim=='uniform'. backend: str. \"cpu\" or \"gpu\" or \"tpu\". Attributes: fit_objs_: dict objects adjusted to each individual time series y_: {array-like} MTS responses (most recent observations first) X_: {array-like} MTS lags xreg_: {array-like} external regressors y_means_: dict a dictionary of each series mean values preds_: {array-like} successive model predictions preds_std_: {array-like} standard deviation around the predictions return_std_: boolean return uncertainty or not (set in predict) df_: data frame the input data frame, in case a data.frame is provided to `fit` Examples: Example 1: import nnetsauce as ns import numpy as np from sklearn import linear_model np.random.seed(123) M = np.random.rand(10, 3) M[:,0] = 10*M[:,0] M[:,2] = 25*M[:,2] print(M) # Adjust Bayesian Ridge regr4 = linear_model.BayesianRidge() obj_MTS = ns.MTS(regr4, lags = 1, n_hidden_features=5) obj_MTS.fit(M) print(obj_MTS.predict()) # with credible intervals print(obj_MTS.predict(return_std=True, level=80)) print(obj_MTS.predict(return_std=True, level=95)) Example 2: import nnetsauce as ns import numpy as np from sklearn import linear_model dataset = { 'date' : ['2001-01-01', '2002-01-01', '2003-01-01', '2004-01-01', '2005-01-01'], 'series1' : [34, 30, 35.6, 33.3, 38.1], 'series2' : [4, 5.5, 5.6, 6.3, 5.1], 'series3' : [100, 100.5, 100.6, 100.2, 100.1]} df = pd.DataFrame(dataset).set_index('date') print(df) # Adjust Bayesian Ridge regr5 = linear_model.BayesianRidge() obj_MTS = ns.MTS(regr5, lags = 1, n_hidden_features=5) obj_MTS.fit(df) print(obj_MTS.predict()) # with credible intervals print(obj_MTS.predict(return_std=True, level=80)) print(obj_MTS.predict(return_std=True, level=95)) [source]","title":"MTS"},{"location":"documentation/time_series/#fit","text":"MTS.fit(X, xreg=None) Fit MTS model to training data X, with optional regressors xreg Parameters: X: {array-like}, shape = [n_samples, n_features] Training time series, where n_samples is the number of samples and n_features is the number of features; X must be in increasing order (most recent observations last) xreg: {array-like}, shape = [n_samples, n_features_xreg] Additional regressors to be passed to obj xreg must be in increasing order (most recent observations last) **kwargs: additional parameters to be passed to self.cook_training_set Returns: self: object [source]","title":"fit"},{"location":"documentation/time_series/#predict","text":"MTS.predict(h=5, level=95, new_xreg=None, **kwargs) Forecast all the time series, h steps ahead Parameters: h: {integer} Forecasting horizon level: {integer} Level of confidence (if obj has option 'return_std' and the posterior is gaussian) new_xreg: {array-like}, shape = [n_samples = h, n_new_xreg] New values of additional (deterministic) regressors on horizon = h new_xreg must be in increasing order (most recent observations last) **kwargs: additional parameters to be passed to self.cook_test_set Returns: model predictions for horizon = h: {array-like}, data frame or tuple. Standard deviation and prediction intervals are returned when `obj.predict` can return standard deviation","title":"predict"},{"location":"examples/classification/","text":"Classification example Other examples can be found here: https://thierrymoudiki.github.io/blog/#QuasiRandomizedNN import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer, load_wine, load_iris, make_classification from sklearn.linear_model import ElasticNet, LinearRegression from sklearn.model_selection import train_test_split from sklearn import metrics from time import time # dataset no. 1 ---------- breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) # Linear Regression is used regr = LinearRegression() fit_obj = ns.MultitaskClassifier(regr, n_hidden_features=5, n_clusters=2, type_clust=\"gmm\") start = time() fit_obj.fit(X_train, y_train) print(time() - start) print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(time() - start) print(metrics.classification_report(preds, y_test))","title":"Classification"},{"location":"examples/classification/#classification-example","text":"Other examples can be found here: https://thierrymoudiki.github.io/blog/#QuasiRandomizedNN import nnetsauce as ns import numpy as np from sklearn.datasets import load_breast_cancer, load_wine, load_iris, make_classification from sklearn.linear_model import ElasticNet, LinearRegression from sklearn.model_selection import train_test_split from sklearn import metrics from time import time # dataset no. 1 ---------- breast_cancer = load_breast_cancer() Z = breast_cancer.data t = breast_cancer.target np.random.seed(123) X_train, X_test, y_train, y_test = train_test_split(Z, t, test_size=0.2) # Linear Regression is used regr = LinearRegression() fit_obj = ns.MultitaskClassifier(regr, n_hidden_features=5, n_clusters=2, type_clust=\"gmm\") start = time() fit_obj.fit(X_train, y_train) print(time() - start) print(fit_obj.score(X_test, y_test)) print(fit_obj.score(X_test, y_test, scoring=\"roc_auc\")) start = time() preds = fit_obj.predict(X_test) print(time() - start) print(metrics.classification_report(preds, y_test))","title":"Classification example"},{"location":"examples/regression/","text":"Regression example Other examples can be found here: https://thierrymoudiki.github.io/blog/#QuasiRandomizedNN import nnetsauce as ns import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, metrics from sklearn import linear_model, gaussian_process # load datasets diabetes = datasets.load_diabetes() X = diabetes.data y = diabetes.target index_train = range(100) index_test = range(100, 125) # layer 1 (base layer) ---- layer1_regr = linear_model.BayesianRidge() layer1_regr.fit(X[index_train,:], y[index_train]) # RMSE score on test set print(np.sqrt(metrics.mean_squared_error(y[index_test], layer1_regr.predict(X[index_test,:])))) # layer 2 using layer 1 ---- layer2_regr = ns.CustomRegressor(obj = layer1_regr, n_hidden_features=3, direct_link=True, bias=True, nodes_sim='sobol', activation_name='tanh', n_clusters=2) layer2_regr.fit(X[index_train,:], y[index_train]) # RMSE score on test set print(np.sqrt(layer2_regr.score(X[index_test,:], y[index_test]))) # layer 3 using layer 2 ---- layer3_regr = ns.CustomRegressor(obj = layer2_regr, n_hidden_features=5, direct_link=True, bias=True, nodes_sim='hammersley', activation_name='sigmoid', n_clusters=2) layer3_regr.fit(X[index_test,:], y[index_test]) # RMSE score on test set print(np.sqrt(layer3_regr.score(X[index_test,:], y[index_test])))","title":"Regression"},{"location":"examples/regression/#regression-example","text":"Other examples can be found here: https://thierrymoudiki.github.io/blog/#QuasiRandomizedNN import nnetsauce as ns import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, metrics from sklearn import linear_model, gaussian_process # load datasets diabetes = datasets.load_diabetes() X = diabetes.data y = diabetes.target index_train = range(100) index_test = range(100, 125) # layer 1 (base layer) ---- layer1_regr = linear_model.BayesianRidge() layer1_regr.fit(X[index_train,:], y[index_train]) # RMSE score on test set print(np.sqrt(metrics.mean_squared_error(y[index_test], layer1_regr.predict(X[index_test,:])))) # layer 2 using layer 1 ---- layer2_regr = ns.CustomRegressor(obj = layer1_regr, n_hidden_features=3, direct_link=True, bias=True, nodes_sim='sobol', activation_name='tanh', n_clusters=2) layer2_regr.fit(X[index_train,:], y[index_train]) # RMSE score on test set print(np.sqrt(layer2_regr.score(X[index_test,:], y[index_test]))) # layer 3 using layer 2 ---- layer3_regr = ns.CustomRegressor(obj = layer2_regr, n_hidden_features=5, direct_link=True, bias=True, nodes_sim='hammersley', activation_name='sigmoid', n_clusters=2) layer3_regr.fit(X[index_test,:], y[index_test]) # RMSE score on test set print(np.sqrt(layer3_regr.score(X[index_test,:], y[index_test])))","title":"Regression example"},{"location":"examples/time_series_examples/","text":"Time series forecasting example Other examples can be found here: https://thierrymoudiki.github.io/blog/#QuasiRandomizedNN import nnetsauce as ns import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, metrics from sklearn import linear_model, gaussian_process np.random.seed(123) M = np.random.rand(10, 3) M[:,0] = 10*M[:,0] M[:,2] = 25*M[:,2] print(M) regr4 = gaussian_process.GaussianProcessRegressor() obj_MTS = ns.MTS(regr4, lags = 1, n_hidden_features=5, bias = False) obj_MTS.fit(M) print(obj_MTS.predict()) # With a deep stack of 'Custom' objects (from previous snippet) obj_MTS2 = ns.MTS(layer3_regr, lags = 1, n_hidden_features=5, bias = False) obj_MTS2.fit(M) print(obj_MTS2.predict()) # Choosing different scalings for the input variables (first input # of tuple 'type_scaling') , hidden layer (second input # of tuple 'type_scaling'), and clustering (third input # of tuple 'type_scaling'). # This is also available for models Base, Custom, etc. # 'minmax', 'minmax', 'std' scalings regr6 = linear_model.BayesianRidge() obj_MTS3 = ns.MTS(regr6, lags = 1, n_hidden_features=2, bias = True, type_scaling = ('minmax', 'minmax', 'std')) obj_MTS3.fit(M) print(obj_MTS3.predict()) # 'minmax', 'standardization', 'minmax' scalings regr7 = linear_model.BayesianRidge() obj_MTS4 = ns.MTS(regr6, lags = 1, n_hidden_features=2, bias = True, type_scaling = ('minmax', 'std', 'minmax')) obj_MTS4.fit(M) print(obj_MTS4.predict())","title":"Time series"},{"location":"examples/time_series_examples/#time-series-forecasting-example","text":"Other examples can be found here: https://thierrymoudiki.github.io/blog/#QuasiRandomizedNN import nnetsauce as ns import numpy as np import matplotlib.pyplot as plt from sklearn import datasets, metrics from sklearn import linear_model, gaussian_process np.random.seed(123) M = np.random.rand(10, 3) M[:,0] = 10*M[:,0] M[:,2] = 25*M[:,2] print(M) regr4 = gaussian_process.GaussianProcessRegressor() obj_MTS = ns.MTS(regr4, lags = 1, n_hidden_features=5, bias = False) obj_MTS.fit(M) print(obj_MTS.predict()) # With a deep stack of 'Custom' objects (from previous snippet) obj_MTS2 = ns.MTS(layer3_regr, lags = 1, n_hidden_features=5, bias = False) obj_MTS2.fit(M) print(obj_MTS2.predict()) # Choosing different scalings for the input variables (first input # of tuple 'type_scaling') , hidden layer (second input # of tuple 'type_scaling'), and clustering (third input # of tuple 'type_scaling'). # This is also available for models Base, Custom, etc. # 'minmax', 'minmax', 'std' scalings regr6 = linear_model.BayesianRidge() obj_MTS3 = ns.MTS(regr6, lags = 1, n_hidden_features=2, bias = True, type_scaling = ('minmax', 'minmax', 'std')) obj_MTS3.fit(M) print(obj_MTS3.predict()) # 'minmax', 'standardization', 'minmax' scalings regr7 = linear_model.BayesianRidge() obj_MTS4 = ns.MTS(regr6, lags = 1, n_hidden_features=2, bias = True, type_scaling = ('minmax', 'std', 'minmax')) obj_MTS4.fit(M) print(obj_MTS4.predict())","title":"Time series forecasting example"}]}